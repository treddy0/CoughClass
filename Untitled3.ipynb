{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa9DRmIQuXDN/bybDdnJ3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treddy0/CoughClass/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libs"
      ],
      "metadata": {
        "id": "oGQ4-3NGgwYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4l95_PSyWDvW"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers import GRU\n",
        "from google.colab import files\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import add, Conv2D,Input,BatchNormalization,TimeDistributed,Embedding,LSTM,GRU,Dense,MaxPooling1D,Dropout,LeakyReLU,ReLU,Flatten,concatenate,Bidirectional\n",
        "from keras.layers import concatenate\n",
        "from keras.models import Model,load_model\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adamax\n",
        "from keras.layers import Input\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.random import normal\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef\n",
        "from sklearn.metrics import cohen_kappa_score,roc_auc_score,confusion_matrix,classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from keras.models import load_model\n",
        "from keras import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pulling Data"
      ],
      "metadata": {
        "id": "Pt5wXkG9SSY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oalei1k0iYmG",
        "outputId": "161fdf92-d2f8-4a3d-db43-64d3723b9111"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt(\"/content/drive/MyDrive/respD/ICBHI_Challenge_diagnosis.txt\", dtype={'names' : ('patient_id', 'disease'), 'formats' : ('i4', 'U10')})"
      ],
      "metadata": {
        "id": "biy8Sg1lkCde"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lot_to_d(list_of_tups):\n",
        "  d = {}\n",
        "  for a,b in list_of_tups:\n",
        "    d[a] = b\n",
        "  return d\n"
      ],
      "metadata": {
        "id": "PZ7bMjEqjPr-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_or_train = lot_to_d(np.loadtxt(\"/content/drive/MyDrive/respD/ICBHI_challenge_train_test.txt\", dtype={'names' : ('fname', 'tt'), 'formats' : ('U30', 'U10')}))"
      ],
      "metadata": {
        "id": "ZOEXbESchSXY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction and Data Augmentation"
      ],
      "metadata": {
        "id": "YLwGFD0se-g4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions augment the data"
      ],
      "metadata": {
        "id": "p2QO_6zLfL95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(data,x):\n",
        "    noise = np.random.randn(len(data))\n",
        "    data_noise = data + x * noise\n",
        "    return data_noise\n",
        "\n",
        "def shift(data,x):\n",
        "    return np.roll(data, x)\n",
        "\n",
        "def stretch(data, rate):\n",
        "    data = librosa.effects.time_stretch(data, rate=rate)\n",
        "    return data"
      ],
      "metadata": {
        "id": "fIU-2v9kjyAC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions reshape the data to pass into model"
      ],
      "metadata": {
        "id": "AhAb48a4fTMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Fix_y(y):\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(y)\n",
        "  encoded_Y = encoder.transform(y)\n",
        "  dummy_y = np_utils.to_categorical(encoded_Y, num_classes=6)\n",
        "  return np.reshape(dummy_y, (dummy_y.shape[0], 1, dummy_y.shape[1])) \n"
      ],
      "metadata": {
        "id": "6gWecvLpXLt9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Fix_X(X):\n",
        "  return np.reshape(X, (X.shape[0], 1, X.shape[1]))"
      ],
      "metadata": {
        "id": "5HQYJpF4XAUg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the original Feature Extraction function"
      ],
      "metadata": {
        "id": "FCk3pGtRfXBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def InstantiateAttributes(dir_):\n",
        "    '''\n",
        "        Extract feature from the Sound data. We extracted Mel-frequency cepstral coefficients( spectral \n",
        "        features ), from the audio data. Augmentation of sound data by adding Noise, streaching and shifting \n",
        "        is also implemented here. 40 features are extracted from each audio data and used to train the model.\n",
        "        Args:\n",
        "            dir_: Input directory to the Sound input file.\n",
        "        Returns:\n",
        "            X_: Array of features extracted from the sound file.\n",
        "            y_: Array of target Labels.\n",
        "    '''\n",
        "    X_=[]\n",
        "    y_=[]\n",
        "    COPD=[]\n",
        "    copd_count=0\n",
        "    for soundDir in (os.listdir(dir_)):\n",
        "        if soundDir[-3:]=='wav'and soundDir[:3]!='103'and soundDir[:3]!='108'and soundDir[:3]!='115':\n",
        "            #data_x, sampling_rate = librosa.load(dir_+soundDir,res_type='kaiser_fast')\n",
        "            #mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "            #X_.append(mfccs)\n",
        "            #y_.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "\n",
        "            p = list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0]\n",
        "            if (p=='COPD'):\n",
        "                if (soundDir[:3] in COPD) and copd_count<2:\n",
        "                    # scipy used here instead of kaiser\n",
        "                    data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                    # print(\"yay\")\n",
        "                    # 40 features are extracted from each audio data and used to train the model.\n",
        "                    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    COPD.append(soundDir[:3])\n",
        "                    copd_count+=1\n",
        "                    X_.append(mfccs)\n",
        "                    y_.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                if (soundDir[:3] not in COPD):\n",
        "                  data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                  mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  COPD.append(soundDir[:3])\n",
        "                  copd_count=0\n",
        "                  X_.append(mfccs)\n",
        "                  y_.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                \n",
        "            if (p!='COPD'):\n",
        "\n",
        "                # augment all non COPD\n",
        "                data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                X_.append(mfccs)\n",
        "                y_.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                \n",
        "                data_noise = add_noise(data_x,0.005)\n",
        "                mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                X_.append(mfccs_noise)\n",
        "                y_.append(p)\n",
        "\n",
        "                data_shift = shift(data_x,1600)\n",
        "                mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                X_.append(mfccs_shift)\n",
        "                y_.append(p)\n",
        "\n",
        "                data_stretch = stretch(data_x,1.2)\n",
        "                mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                X_.append(mfccs_stretch)\n",
        "                y_.append(p)\n",
        "                \n",
        "                data_stretch_2 = stretch(data_x,0.8)\n",
        "                mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                X_.append(mfccs_stretch_2)\n",
        "                y_.append(p)\n",
        "\n",
        "    X_ = np.array(X_)\n",
        "    y_ = np.array(y_)\n",
        "    \n",
        "    return X_,y_"
      ],
      "metadata": {
        "id": "VJe6fLx4kLlt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is th modified feature extraction function"
      ],
      "metadata": {
        "id": "oAF4hhSvfafn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SplitInstantiateAttributes(dir_, test_split_size):\n",
        "\n",
        "    X_train=[]\n",
        "    y_train=[]\n",
        "    X_test=[]\n",
        "    y_test=[]\n",
        "    COPD=[]\n",
        "    copd_count=0\n",
        "    for soundDir in (os.listdir(dir_)):\n",
        "        if soundDir[-3:]=='wav'and soundDir[:3]!='103'and soundDir[:3]!='108'and soundDir[:3]!='115':\n",
        "            #data_x, sampling_rate = librosa.load(dir_+soundDir,res_type='kaiser_fast')\n",
        "            #mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "            #X_.append(mfccs)\n",
        "            #y_.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "\n",
        "            p = list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0]\n",
        "            if (p=='COPD'):\n",
        "                if (soundDir[:3] in COPD) and copd_count<2:\n",
        "                    # scipy resample instead of kaiser\n",
        "                    data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                    # print(\"yay\")\n",
        "                    # 40 features are extracted from each audio data and used to train the model.\n",
        "                    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    COPD.append(soundDir[:3])\n",
        "                    copd_count+=1\n",
        "                    if random.random() < test_split_size:\n",
        "                      X_test.append(mfccs)\n",
        "                      y_test.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                    else:\n",
        "                      X_train.append(mfccs)\n",
        "                      y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                if (soundDir[:3] not in COPD):\n",
        "                  data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                  mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  COPD.append(soundDir[:3])\n",
        "                  copd_count=0\n",
        "                  if random.random() < test_split_size:\n",
        "                      X_test.append(mfccs)\n",
        "                      y_test.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                  else:\n",
        "                      X_train.append(mfccs)\n",
        "                      y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                \n",
        "            if (p!='COPD'):\n",
        "                data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0)\n",
        "                #make sure augmented go into same split\n",
        "                if random.random() < test_split_size:\n",
        "                  X_test.append(mfccs)\n",
        "                  y_test.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "              \n",
        "                  data_noise = add_noise(data_x,0.005)\n",
        "                  mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_test.append(mfccs_noise)\n",
        "                  y_test.append(p)\n",
        "\n",
        "                  data_shift = shift(data_x,1600)\n",
        "                  mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_test.append(mfccs_shift)\n",
        "                  y_test.append(p)\n",
        "\n",
        "                  data_stretch = stretch(data_x,1.2)\n",
        "                  mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_test.append(mfccs_stretch)\n",
        "                  y_test.append(p)\n",
        "                  \n",
        "                  data_stretch_2 = stretch(data_x,0.8)\n",
        "                  mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_test.append(mfccs_stretch_2)\n",
        "                  y_test.append(p)\n",
        "                else:\n",
        "                  X_train.append(mfccs)\n",
        "                  y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "              \n",
        "                  data_noise = add_noise(data_x,0.005)\n",
        "                  mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_train.append(mfccs_noise)\n",
        "                  y_train.append(p)\n",
        "\n",
        "                  data_shift = shift(data_x,1600)\n",
        "                  mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_train.append(mfccs_shift)\n",
        "                  y_train.append(p)\n",
        "\n",
        "                  data_stretch = stretch(data_x,1.2)\n",
        "                  mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_train.append(mfccs_stretch)\n",
        "                  y_train.append(p)\n",
        "                  \n",
        "                  data_stretch_2 = stretch(data_x,0.8)\n",
        "                  mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  X_train.append(mfccs_stretch_2)\n",
        "                  y_train.append(p)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_tesr = np.array(y_test)\n",
        "    \n",
        "    return X_train,y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "djPbqkHOTWCe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the corrected feature extraction function"
      ],
      "metadata": {
        "id": "PbnWW9IWfeZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OGSplitInstantiateAttributes(dir_):\n",
        "\n",
        "    X_train=[]\n",
        "    y_train=[]\n",
        "    X_test=[]\n",
        "    y_test=[]\n",
        "    COPD=[]\n",
        "    copd_count=0\n",
        "    for soundDir in (os.listdir(dir_)):\n",
        "        if soundDir[-3:]=='wav'and soundDir[:3]!='103'and soundDir[:3]!='108'and soundDir[:3]!='115':\n",
        "            #data_x, sampling_rate = librosa.load(dir_+soundDir,res_type='kaiser_fast')\n",
        "            #mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "            #X_.append(mfccs)\n",
        "            #y_.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "\n",
        "            p = list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0]\n",
        "            if (p=='COPD'):\n",
        "                if (soundDir[:3] in COPD) and copd_count<2:\n",
        "                    data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                    # print(\"yay\")\n",
        "                    # 40 features are extracted from each audio data and used to train the model.\n",
        "                    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    COPD.append(soundDir[:3])\n",
        "                    copd_count+=1\n",
        "                    # check if patient is marked as test or train\n",
        "                    if soundDir[:-4] in test_or_train:\n",
        "                      if test_or_train[soundDir[:-4]] == \"test\":\n",
        "                        X_test.append(mfccs)\n",
        "                        y_test.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                      elif test_or_train[soundDir[:-4]] == \"train\":\n",
        "                        X_train.append(mfccs)\n",
        "                        y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                    else:\n",
        "                      X_train.append(mfccs)\n",
        "                      y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                if (soundDir[:3] not in COPD):\n",
        "                  data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                  mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                  COPD.append(soundDir[:3])\n",
        "                  copd_count=0\n",
        "                  # check if patient is marked as test or train\n",
        "                  if soundDir[:-4] in test_or_train:\n",
        "                    if test_or_train[soundDir[:-4]] == \"test\":\n",
        "                        X_test.append(mfccs)\n",
        "                        y_test.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                    elif test_or_train[soundDir[:-4]] == \"train\":\n",
        "                        X_train.append(mfccs)\n",
        "                        y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                  else:\n",
        "                      X_train.append(mfccs)\n",
        "                      y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                \n",
        "            if (p!='COPD'):\n",
        "                data_x, sampling_rate = librosa.load(dir_+soundDir, res_type='scipy')\n",
        "                mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=40).T,axis=0)\n",
        "                # check if patient is marked as test or train\n",
        "                if soundDir[:-4] in test_or_train:\n",
        "                  if test_or_train[soundDir[:-4]] == \"test\":\n",
        "                    X_test.append(mfccs)\n",
        "                    y_test.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                \n",
        "                    data_noise = add_noise(data_x,0.005)\n",
        "                    mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_test.append(mfccs_noise)\n",
        "                    y_test.append(p)\n",
        "\n",
        "                    data_shift = shift(data_x,1600)\n",
        "                    mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_test.append(mfccs_shift)\n",
        "                    y_test.append(p)\n",
        "\n",
        "                    data_stretch = stretch(data_x,1.2)\n",
        "                    mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_test.append(mfccs_stretch)\n",
        "                    y_test.append(p)\n",
        "                    \n",
        "                    data_stretch_2 = stretch(data_x,0.8)\n",
        "                    mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_test.append(mfccs_stretch_2)\n",
        "                    y_test.append(p)\n",
        "                  elif test_or_train[soundDir[:-4]] == \"train\":\n",
        "                    X_train.append(mfccs)\n",
        "                    y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "                \n",
        "                    data_noise = add_noise(data_x,0.005)\n",
        "                    mfccs_noise = np.mean(librosa.feature.mfcc(y=data_noise, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_train.append(mfccs_noise)\n",
        "                    y_train.append(p)\n",
        "\n",
        "                    data_shift = shift(data_x,1600)\n",
        "                    mfccs_shift = np.mean(librosa.feature.mfcc(y=data_shift, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_train.append(mfccs_shift)\n",
        "                    y_train.append(p)\n",
        "\n",
        "                    data_stretch = stretch(data_x,1.2)\n",
        "                    mfccs_stretch = np.mean(librosa.feature.mfcc(y=data_stretch, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_train.append(mfccs_stretch)\n",
        "                    y_train.append(p)\n",
        "                    \n",
        "                    data_stretch_2 = stretch(data_x,0.8)\n",
        "                    mfccs_stretch_2 = np.mean(librosa.feature.mfcc(y=data_stretch_2, sr=sampling_rate, n_mfcc=40).T,axis=0) \n",
        "                    X_train.append(mfccs_stretch_2)\n",
        "                    y_train.append(p)\n",
        "                else:\n",
        "                    X_train.append(mfccs)\n",
        "                    y_train.append(list(data[data['patient_id']==int(soundDir[:3])]['disease'])[0])\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_tesr = np.array(y_test)\n",
        "    \n",
        "    return X_train,y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "PqVfGAgzionM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_, y_ = InstantiateAttributes('/content/drive/MyDrive/respD/ICBHI_final_database/')"
      ],
      "metadata": {
        "id": "OUO5BYuskXCu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ = Fix_X(X_)\n",
        "y_ = Fix_y(y_)"
      ],
      "metadata": {
        "id": "OZeXZ5OOQCMA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_1, y_train_1, X_test_1, y_test_1 = SplitInstantiateAttributes('/content/drive/MyDrive/respD/ICBHI_final_database/', 0.25)"
      ],
      "metadata": {
        "id": "3CG7xde-WfXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2, y_train_2, X_test_2, y_test_2 = OGSplitInstantiateAttributes('/content/drive/MyDrive/respD/ICBHI_final_database/')"
      ],
      "metadata": {
        "id": "JrYqBTFFn3kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_1 = Fix_X(X_train_1)\n",
        "X_test_1 = Fix_X(X_test_1)\n",
        "y_train_1 = Fix_y(y_train_1)\n",
        "y_test_1 = Fix_y(y_test_1)\n",
        "\n",
        "X_train_2 = Fix_X(X_train_2)\n",
        "X_test_2 = Fix_X(X_test_2)\n",
        "y_train_2 = Fix_y(y_train_2)\n",
        "y_test_2 = Fix_y(y_test_2)"
      ],
      "metadata": {
        "id": "Kx-CP8_FYEoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "iMI6NjsfSiA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def InstantiateModel(in_):\n",
        "   '''\n",
        "      Architecture of the Deep Learning Model.\n",
        "      Args:\n",
        "        in_: input tensor shape\n",
        "      Returns: Tensor model\n",
        "   '''\n",
        "   model_2_1 = GRU(32,return_sequences=True,activation=None,go_backwards=True)(in_)\n",
        "   model_2 = LeakyReLU()(model_2_1)\n",
        "   model_2 = GRU(128,return_sequences=True, activation=None,go_backwards=True)(model_2)\n",
        "    #model_2 = BatchNormalization()(model_2)\n",
        "   model_2 = LeakyReLU()(model_2)\n",
        "    \n",
        "   model_3 = GRU(64,return_sequences=True,activation=None,go_backwards=True)(in_)\n",
        "   model_3 = LeakyReLU()(model_3)\n",
        "   model_3 = GRU(128,return_sequences=True, activation=None,go_backwards=True)(model_3)\n",
        "    #model_3 = BatchNormalization()(model_3)\n",
        "   model_3 = LeakyReLU()(model_3)\n",
        "    \n",
        "   model_add_1 = add([model_3,model_2])\n",
        "    \n",
        "   model_5 = GRU(128,return_sequences=True,activation=None,go_backwards=True)(model_add_1)\n",
        "   model_5 = LeakyReLU()(model_5)\n",
        "   model_5 = GRU(32,return_sequences=True, activation=None,go_backwards=True)(model_5)\n",
        "   model_5 = LeakyReLU()(model_5)\n",
        "    \n",
        "   model_6 = GRU(64,return_sequences=True,activation=None,go_backwards=True)(model_add_1)\n",
        "   model_6 = LeakyReLU()(model_6)\n",
        "   model_6 = GRU(32,return_sequences=True, activation=None,go_backwards=True)(model_6)\n",
        "   model_6 = LeakyReLU()(model_6)\n",
        "    \n",
        "   model_add_2 = add([model_5,model_6,model_2_1])\n",
        "    \n",
        "    \n",
        "   model_7 = Dense(64, activation=None)(model_add_2)\n",
        "   model_7 = LeakyReLU()(model_7)\n",
        "   model_7 = Dropout(0.2)(model_7)\n",
        "   model_7 = Dense(16, activation=None)(model_7)\n",
        "   model_7 = LeakyReLU()(model_7)\n",
        "    \n",
        "   model_9 = Dense(32, activation=None)(model_add_2)\n",
        "   model_9 = LeakyReLU()(model_9)\n",
        "   model_9 = Dropout(0.2)(model_9)\n",
        "   model_9 = Dense(16, activation=None)(model_9)\n",
        "   model_9 = LeakyReLU()(model_9)\n",
        "    \n",
        "   model_add_3 = add([model_7,model_9])\n",
        "\n",
        "   model_10 = Dense(16, activation=None)(model_add_3)\n",
        "    #model_10 = BatchNormalization()(model_10)\n",
        "   model_10 = LeakyReLU()(model_10)\n",
        "   model_10 = Dropout(0.5)(model_10)\n",
        "    #Model_7 = MaxPooling1D(pool_size=2)(mode)\n",
        "   model_10 = Dense(6, activation=\"softmax\")(model_10)\n",
        "    \n",
        "   return model_10"
      ],
      "metadata": {
        "id": "6KVFSF0EmAsl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(X_train,  X_test, y_train, y_test):\n",
        "    '''\n",
        "        Training the Neural Network model against the data.\n",
        "        Args: \n",
        "            X: Array of features to be trained.\n",
        "            y: Array of Target attribute.\n",
        "\n",
        "        Returns: Save Trained model weights.\n",
        "    '''\n",
        "    K.clear_session()\n",
        "    batch_size=X_train.shape[0]\n",
        "    time_steps=X_train.shape[1]\n",
        "    data_dim=X_train.shape[2]\n",
        "    # data_dim = 40\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    batch_size = X_train.shape[0]\n",
        "    \n",
        "\n",
        "    # Input_Sample = Input(shape=(1, 40))\n",
        "    Input_Sample = Input(shape=(time_steps,data_dim))\n",
        "    # Input_Sample = normal([40, 1, ])\n",
        "    Output_ = InstantiateModel(Input_Sample)\n",
        "    print(Input_Sample.shape)\n",
        "    print(Output_.shape)\n",
        "    Model_Enhancer = Model(inputs=Input_Sample, outputs=Output_)\n",
        "    print(\"ckpt 1\")\n",
        "\n",
        "    Model_Enhancer.compile(loss='categorical_crossentropy', metrics=['accuracy', metrics.AUC(), metrics.CategoricalAccuracy()], optimizer=Adamax(learning_rate=0.0025))\n",
        "    print(\"ckpt 2\")\n",
        "\n",
        "    ES = EarlyStopping(monitor='val_loss', min_delta=0.5, patience=200, verbose=1, mode='auto', baseline=None,\n",
        "                              restore_best_weights=False)\n",
        "    MC = ModelCheckpoint('/content/drive/MyDrive/respD/best_model.h5', monitor='val_categorical_accuracy', mode='auto', verbose=1, save_best_only=True)\n",
        "    \n",
        "    # class_weights = class_weight.compute_sample_weight('balanced',\n",
        "    #                                                 y[:,0],\n",
        "    #                                                 indices=np.unique(y[:,0],axis=0))\n",
        "    ModelHistory = Model_Enhancer.fit(X_train, y_train, batch_size=batch_size, epochs=2500,\n",
        "                                  validation_data=(X_test, y_test),\n",
        "                                  callbacks = [MC],\n",
        "                                  #class_weight=class_weights,\n",
        "                                  verbose=1)\n",
        "    \n",
        "    return ModelHistory"
      ],
      "metadata": {
        "id": "DUJw717iNyxY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "7w74umOTgT4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This runs the model on the original pipeline"
      ],
      "metadata": {
        "id": "N9fcpcBggX3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.25, random_state=42)\n",
        "history = trainModel(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "wDDS5PoI7W03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This runs the model on the modified pipeline"
      ],
      "metadata": {
        "id": "jBTDqt55gdMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = trainModel(X_train_1,  X_test_1, y_train_1, y_test_1)"
      ],
      "metadata": {
        "id": "Z61XJrdikl2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This runs the model on the corrected pipeline"
      ],
      "metadata": {
        "id": "AvBJ7u1uggMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = trainModel(X_train_2,  X_test_2, y_train_2, y_test_2)"
      ],
      "metadata": {
        "id": "hu7KxN_9eljO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')"
      ],
      "metadata": {
        "id": "Zqu9qaBrxz9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')"
      ],
      "metadata": {
        "id": "hznruD0F-y3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.title('model auc')\n",
        "plt.ylabel('auc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')"
      ],
      "metadata": {
        "id": "gUKLtD8C4yd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Saved Model"
      ],
      "metadata": {
        "id": "wgiguwUkg-CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validateModel(X_val):\n",
        "\t'''\n",
        "\t   Validate the performance of the Model by loading the trained model weights.\n",
        "\t   Args:\n",
        "\t       X_val : Array for features.\n",
        "\t   Returns: Model prediction against the input features.\n",
        "\t'''\n",
        "\tModel_Loaded = load_model('/content/drive/MyDrive/respD/best_model.h5')\n",
        "\n",
        "\tyhat_probs = Model_Loaded.predict(X_val, verbose=1)\n",
        "\tyhat_probs = yhat_probs.reshape(yhat_probs.shape[0],yhat_probs.shape[2])\n",
        "\tyhat_classes =np.argmax(yhat_probs,axis=1)\n",
        "\n",
        "\treturn yhat_classes"
      ],
      "metadata": {
        "id": "hzJecgW45zFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalModel(y_test, y_pred):\n",
        "  y_test = y_test.reshape(y_test.shape[0],y_test.shape[2])\n",
        "  y_test =np.argmax(y_test,axis=1)\n",
        "\n",
        "    # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "\t# precision tp / (tp + fp)\n",
        "  precision = precision_score(y_test, y_pred,average='weighted')\n",
        "  print('Precision: %f' % precision)\n",
        "\t# recall: tp / (tp + fn)\n",
        "  recall = recall_score(y_test, y_pred,average='weighted')\n",
        "  print('Recall: %f' % recall)\n",
        "\t# f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(y_test, y_pred,average='weighted')\n",
        "  print('F1 score: %f' % f1)\n",
        "\t \n",
        "\t# kappa\n",
        "  kappa = cohen_kappa_score(y_test, y_pred)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  MatthewsCorrCoef = matthews_corrcoef(y_test, y_pred)\n",
        "  print('Matthews correlation coefficient: %f' % MatthewsCorrCoef)\n",
        "\t# ROC AUC\n",
        "\t# confusion matrix\n",
        "  matrix = classification_report(y_test, y_pred)\n",
        "  print(matrix)\n",
        "\n",
        "  return {\n",
        "\t       \"Accuracy\": accuracy,\n",
        "\t       \"Precision\": precision,\n",
        "\t       \"Recall\": recall,\n",
        "\t       \"F1 score\": f1,\n",
        "\t       \"Cohens kappa\": kappa,\n",
        "\t       \"Matthews correlation coefficient\": MatthewsCorrCoef\n",
        "  }"
      ],
      "metadata": {
        "id": "rbekS2ba6njs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalModel(y_test, validateModel(X_test))"
      ],
      "metadata": {
        "id": "WFdlaN9JD7iL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}